{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('https://rd.springer.com/journal/10551', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_url = 'https://rd.springer.com/journal/10551/volume/%d/toc' %10\n",
    "\n",
    "\n",
    "r = requests.get(volume_url, \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://rd.springer.com'\n",
    "issue = './/a[@class=\"title\"]/@href'\n",
    "issue_urls = [base_url + i for i in response.xpath(issue).extract()]\n",
    "issue_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_url = 'https://rd.springer.com/journal/10551/10/5/page/1'\n",
    "r = requests.get(issue_url, \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axpath = './/h3//a/@href'\n",
    "article_urls = [ base_url + i for i in response.xpath(axpath).extract()]\n",
    "article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('https://rd.springer.com/article/10.1007/s10551-015-2664-7', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "from scrapy import Item, Field\n",
    "\n",
    "\n",
    "class DocumentItem(Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    abstract = Field()\n",
    "\n",
    "    publication_date = Field()\n",
    "    submission_date = Field()\n",
    "    online_date = Field()\n",
    "    revision_date = Field()\n",
    "    accepted_date = Field()\n",
    "\n",
    "    title = Field()\n",
    "    coverpage_url = Field()\n",
    "    fpage = Field()\n",
    "    lpage = Field()\n",
    "    pages = Field()\n",
    "    submission_path = Field()\n",
    "\n",
    "    publication_title = Field()\n",
    "\n",
    "\n",
    "class KeywordItem(Item):\n",
    "    keyword = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "\n",
    "class SourceItem(Item):\n",
    "    publication_title = Field()\n",
    "    chief_editor = Field()\n",
    "    issn = Field()\n",
    "    description = Field()\n",
    "    home_url = Field()\n",
    "    coverimage = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "class AuthorItem(Item):\n",
    "    institution = Field()\n",
    "    email = Field()\n",
    "    avatar = Field()\n",
    "    vitae = Field()\n",
    "    fname = Field()\n",
    "    lname = Field()\n",
    "    address = Field()\n",
    "\n",
    "    title = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper\n",
    "document = dict(\n",
    "    title = './/h1[@class=\"ArticleTitle\"]/text()',\n",
    "    abstract = './/p[@id=\"Par1\"]/text()',\n",
    "    online_date = './/dd[@class=/a[@class=\"gtm-first-online\"]/text()',\n",
    "    meta = './/p[@class=\"icon--meta-keyline-before\"]/*/text()',\n",
    "    publication_date = './/p[@class=\"icon--meta-keyline-before\"]/span/time/@datetime'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.get_xpath(document['publication_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "l = ItemLoader(item = DocumentItem(), response = response)\n",
    "l.default_output_processor = TakeFirst()\n",
    "\n",
    "\n",
    "meta = l.get_xpath(document['meta'])\n",
    "\n",
    "submission_path = meta[1] + meta[2]\n",
    "submission_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "def load_document(response, document):\n",
    "    l = ItemLoader(item = DocumentItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    \n",
    "    l.add_value('coverpage_url', response.url)\n",
    "    l.add_xpath('abstract', document['abstract'])\n",
    "    l.add_xpath('title', document['title'])\n",
    "    try:\n",
    "        meta = l.get_xpath(document['meta'])\n",
    "        l.add_value('submission_path', meta[1] + meta[2])\n",
    "        pages = meta[-1].split(' ')[-1]\n",
    "        if '–' in pages:\n",
    "            fp = int(pages.split('–')[0])\n",
    "            lp = int(pages.split('–')[1])\n",
    "        elif '-' in pages:\n",
    "            fp = int(pages.split('-')[0])\n",
    "            lp = int(pages.split('-')[1])\n",
    "        l.add_value('fpage', fp)\n",
    "        l.add_value('lpage', lp)\n",
    "        l.add_value('pages', lp-fp+1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    l.add_value('publication_date', parse(response.xpath(document['publication_date']).extract()[0]))\n",
    "\n",
    "    # mark it down, with source's publication_title\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_document(response, document)\n",
    "l.load_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = './/span[@class=\"Keyword\"]/text()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ normalize('NFKD', i)  for i in response.xpath(keyword).extract()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = dict(\n",
    "    auths = './/ul[@class=\"test-contributor-names\"]',\n",
    "    auth = './li',\n",
    "    name = './/span[@class=\"authors-affiliations__name\"]/text()',\n",
    "    email = './/a[@class=\"gtm-email-author\"]/@title',\n",
    "    affil_id = './/li[@data-affiliation]/@data-affiliation',\n",
    "    affiliation = './/*[@id=\"%s\"]',\n",
    "    department = './/*[@class=\"affiliation__department\"]/text()',\n",
    "    university = './/*[@class=\"affiliation__name\"]/text()',\n",
    "    city = './/*[@class=\"affiliation__city\"]/text()',\n",
    "    country = './/*[@class=\"affiliation__country\"]/text()'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths = response.xpath(author['auths'])[-1]\n",
    "auths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths.xpath(author['auth']).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = auths.xpath(author['auth'])[0]\n",
    "auth.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def load_author(response, author):\n",
    "    auths = response.xpath(author['auths'])[-1]\n",
    "    for auth in auths.xpath(author['auth']):\n",
    "        l = ItemLoader(item = AuthorItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "\n",
    "        # author's first name and last name\n",
    "        try:\n",
    "            name = normalize('NFKD', auth.xpath(author['name']).extract()[0])\n",
    "            fn = name.split()[0]\n",
    "            ln = name.split()[-1]\n",
    "            l.add_value('fname', fn)\n",
    "            l.add_value('lname', ln)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's email\n",
    "        try:\n",
    "            email = auth.xpath(author['email']).extract()[0]\n",
    "            l.add_value('email', email)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's institution\n",
    "        try:\n",
    "            affil_id = auth.xpath(author['affil_id']).extract()[0][1:]\n",
    "            affiliation = response.xpath(author['affiliation'] %(affil_id))[0]\n",
    "            # department\n",
    "            department = affiliation.xpath(author['department']).extract()[0]\n",
    "            # university\n",
    "            university = affiliation.xpath(author['university']).extract()[0]\n",
    "            institution = department +' ' +  university\n",
    "            l.add_value('institution', institution)\n",
    "\n",
    "            # address\n",
    "            city = affiliation.xpath(author['city']).extract()[0]\n",
    "            country = affiliation.xpath(author['country']).extract()[0]\n",
    "            address = city + ' ' +  country\n",
    "            l.add_value('address', address)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        yield l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(load_author(response, author))\n",
    "for i in l:\n",
    "    print(i.load_item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affilication = response.xpath('.//*[@id=\"affiliation-1\"]')\n",
    "affilication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auths = response.xpath(author['auths'])[-1]\n",
    "auths.xpath(author['auth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = './/ul[@class=\"test-contributor-names\"]'\n",
    "auths = response.xpath(authors)[-1]\n",
    "auths\n",
    "author['auths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selector = response.xpath(author)[-1]\n",
    "auth = Selector.xpath('./li')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = normalize('NFKD', auth.xpath(author['name']).extract()[0])\n",
    "fn = name.split()[0]\n",
    "ln = name.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = name.split()[0]\n",
    "lname = name.split()[-1]\n",
    "print(fname)\n",
    "print(lname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.xpath('.//a[@class=\"gtm-email-author\"]/@title').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliation = fir.xpath('.//li[@data-affiliation]/@data-affiliation').extract()[0][1:]\n",
    "affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = response.xpath('.//*[@id=\"affiliation-1\"]')\n",
    "fill.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = response.xpath('.//*[@id=\"affiliation-1\"]')\n",
    "\n",
    "# insitution\n",
    "# department\n",
    "department = fill.xpath('.//*[@class=\"affiliation__department\"]/text()').extract()[0]\n",
    "\n",
    "# university\n",
    "university = fill.xpath('.//*[@class=\"affiliation__name\"]/text()').extract()[0]\n",
    "institution = department +' ' +  university\n",
    "\n",
    "\n",
    "# address\n",
    "city = fill.xpath('.//*[@class=\"affiliation__city\"]/text()').extract()[0]\n",
    "country = fill.xpath('.//*[@class=\"affiliation__country\"]/text()').extract()[0]\n",
    "address = city +' ' +  country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "normalize(\"NFKD\",'Heather\\xa0R.\\xa0Dixon-Fowler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ normalize('NFKD',i) for i in Selector.xpath('.//span[@class=\"authors-affiliations__name\"]/text()').extract()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "chart = HTML(fill)\n",
    "display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = HTML(a[1])\n",
    "display(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('https://link.springer.com/journal/41267', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dict(\n",
    "    issn = '//span[@class=\"pissn\"]/text()',\n",
    "    publication_title = './/h1[@id=\"title\"]/text()',\n",
    "    description = './/div[@class=\"abstract-content formatted\"]/p/text()',\n",
    "    coverimage = './/img[@class=\"look-inside-cover\"]/@src'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath(source['description']).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "def load_source(response, source):\n",
    "    l = ItemLoader(item = SourceItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    l.add_xpath(\"issn\", source['issn'])\n",
    "    l.add_xpath('publication_title', source['publication_title'])\n",
    "    l.add_xpath('coverimage', source['coverimage'])\n",
    "    l.add_xpath('description',source['description'], Join())\n",
    "    l.add_value('home_url', response.url)\n",
    "    publication_title = l.get_xpath(source['publication_title'])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_source(response, source)\n",
    "l.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
