{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('http://journals.sagepub.com/doi/full/10.1177/0018726716650730', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# http://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "from scrapy import Item, Field\n",
    "\n",
    "\n",
    "class DocumentItem(Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    abstract = Field()\n",
    "\n",
    "    publication_date = Field()\n",
    "    submission_date = Field()\n",
    "    online_date = Field()\n",
    "    revision_date = Field()\n",
    "    accepted_date = Field()\n",
    "\n",
    "    title = Field()\n",
    "    coverpage_url = Field()\n",
    "    fpage = Field()\n",
    "    lpage = Field()\n",
    "    pages = Field()\n",
    "    submission_path = Field()\n",
    "\n",
    "    publication_title = Field()\n",
    "\n",
    "\n",
    "class KeywordItem(Item):\n",
    "    keyword = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "\n",
    "class SourceItem(Item):\n",
    "    publication_title = Field()\n",
    "    chief_editor = Field()\n",
    "    issn = Field()\n",
    "    description = Field()\n",
    "    home_url = Field()\n",
    "    coverimage = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "class AuthorItem(Item):\n",
    "    institution = Field()\n",
    "    email = Field()\n",
    "    avatar = Field()\n",
    "    vitae = Field()\n",
    "    fname = Field()\n",
    "    lname = Field()\n",
    "    address = Field()\n",
    "\n",
    "    title = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagepub\n",
    "document = dict(\n",
    "    title = './/div[@class=\"publicationContentTitle\"]/h1/text()',\n",
    "    abstract = './/div[@class=\"abstractSection abstractInFull\"]/p/text()',\n",
    "    submission_path = './/div[@class=\"articleJournalNavTitle\"]/text()',\n",
    "    pages = './/div[@class=\"Article information\"]/div/text()',\n",
    "    dates = './/div[@class=\"published-dates\"]/text()'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "def load_document(response, document):\n",
    "    l = ItemLoader(item = DocumentItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    \n",
    "    l.add_value('coverpage_url', response.url)\n",
    "    l.add_xpath('abstract', document['abstract'])\n",
    "    l.add_value('title', (l.get_xpath(document['title'])[0]).replace('\\n', '').strip())\n",
    "    l.add_value('submission_path', l.get_xpath(document['submission_path'])[0].replace('\\n', '').strip())\n",
    "\n",
    "    # handle dates\n",
    "    try:\n",
    "        dates =[i.replace('\\n', '').replace(';', '').strip() for i in response.xpath(document['dates']).extract()[-2:]]\n",
    "        d = [parse(i) for i in dates]\n",
    "        l.add_value('online_date', d[0])\n",
    "        l.add_value('publication_date', d[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # handle pages\n",
    "    try:\n",
    "        pages = response.xpath(document['pages']).extract()[0].strip().split('\\n')[-1].strip().split(':')[-1]\n",
    "        if '–' in pages:\n",
    "            fp = int(pages.split('–')[0])\n",
    "            lp = int(pages.split('–')[1])\n",
    "        elif '-' in pages:\n",
    "            fp = int(pages.split('-')[0])\n",
    "            lp = int(pages.split('-')[1])\n",
    "        l.add_value('fpage', fp)\n",
    "        l.add_value('lpage', lp)\n",
    "        l.add_value('pages', lp-fp+1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # mark it down, with source's publication_title\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_document(response, document)\n",
    "l.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \n",
    "\n",
    "\n",
    "def load_keyword(response, keyword):\n",
    "    for k in response.xpath(keyword).extract():\n",
    "        k1 = k.split(';')[0]\n",
    "        l = ItemLoader(item = KeywordItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "        l.add_value('keyword', k1)\n",
    "        yield l\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author =dict(\n",
    "    auth = './/div[@class=\"contribDegrees\"]',\n",
    "    name = './/a[@class=\"entryAuthor\"]/text()',\n",
    "    email = './/a[@class=\"email\"]/@href',\n",
    "    institution = './/div[@class=\"artice-info-affiliation\"]/text()'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_author(response,author):\n",
    "    auths = response.xpath(author['auth'])\n",
    "    for auth in auths:\n",
    "        l = ItemLoader(item = AuthorItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "\n",
    "        # author's first name and last name\n",
    "        name = auth.xpath(author['name']).extract()[0].split()\n",
    "        fn = name[0]\n",
    "        ln = name[-1]\n",
    "        l.add_value('fname', fn)\n",
    "        l.add_value('lname', ln)\n",
    "\n",
    "        # author's email\n",
    "        try:\n",
    "            email = auth.xpath(author['email']).extract()[0][7:]\n",
    "            l.add_value('email', email)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # author's institution\n",
    "        try:\n",
    "            institution = auth.xpath(author['institution']).extract()[0]\n",
    "            l.add_value('institution', institution)\n",
    "        except:\n",
    "            pass\n",
    "        yield l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = load_author(response, author)\n",
    "for i in t:\n",
    "    print(i.load_item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('https://us.sagepub.com/en-us/nam/human-relations/journal200870', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dict(\n",
    "    issn = '//span[@class=\"margin-right\"]/text()',\n",
    "    chief_editor = '//td[@class=\"journal-contributor-member\"]/a/text()',\n",
    "    publication_title = './/h1[@class=\"heading-large heading-spacing--small\"]/text()',\n",
    "    description = './/div[@class=\"field field-name-field-website-configuration field-type-text-long field-label-hidden\"]',\n",
    "    coverimage = './/img[@class=\"sage-thumbnail-width-150px lazy\"]/@data-original'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scrapy.loader.processors import Join, TakeFirst, Join\n",
    "\n",
    "\n",
    "# this function is used to strip the html tags\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def load_source(response, source):\n",
    "    website_url = 'https://us.sagepub.com' \n",
    "    l = ItemLoader(item = SourceItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    l.add_value(\"issn\", response.xpath(source['issn']).extract()[1].split()[-1])\n",
    "    l.add_value('chief_editor', response.xpath(source['chief_editor']).extract()[0])\n",
    "    l.add_xpath('publication_title', source['publication_title'])\n",
    "    l.add_value('coverimage', website_url + l.get_xpath(source['coverimage'])[0])\n",
    "    l.add_xpath('description', './/div[@class=\"field-item even\"]', Join(), cleanhtml, lambda x: x.replace('\\n', '').replace('  ', '').strip())\n",
    "    l.add_value('home_url', response.url)\n",
    "    publication_title = l.get_xpath(source['publication_title'])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_source(response, source)\n",
    "l.load_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('https://us.sagepub.com/en-us/nam/human-relations/journal200870', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_prefix_url = 'http://journals.sagepub.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://journals.sagepub.com/toc/huma/1', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_url = './/a[@class=\"ref nowrap\"]/@href'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ base_url + i for i in response.xpath(article_url).extract()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in response.xpath(article_url).extract()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc_prefix_url + i  for i in response.xpath(document_url).extract()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
