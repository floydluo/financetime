{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need headers to disguise our bot as a browser\n",
    "\n",
    "headers = {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.131 Safari/537.36\",\n",
    "    \"Accept-Encoding\": \"gzip,deflate,sdch\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.8,en-US;q=0.6,en;q=0.4,zh-TW;q=0.2\",\n",
    "}\n",
    "\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "r = requests.get('http://pubsonline.informs.org/doi/abs/10.1287/mnsc.2015.2304', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Item, Field\n",
    "\n",
    "class DocumentItem(Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    abstract = Field()\n",
    "\n",
    "    publication_date = Field()\n",
    "    submission_date = Field()\n",
    "    online_date = Field()\n",
    "    revision_date = Field()\n",
    "    accepted_date = Field()\n",
    "\n",
    "    title = Field()\n",
    "    coverpage_url = Field()\n",
    "    fpage = Field()\n",
    "    lpage = Field()\n",
    "    pages = Field()\n",
    "    submission_path = Field()\n",
    "\n",
    "    publication_title = Field()\n",
    "\n",
    "class KeywordItem(Item):\n",
    "    keyword = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "class SourceItem(Item):\n",
    "    publication_title = Field()\n",
    "    chief_editor = Field()\n",
    "    issn = Field()\n",
    "    description = Field()\n",
    "    home_url = Field()\n",
    "    coverimage = Field()\n",
    "\n",
    "    title = Field()\n",
    "\n",
    "class AuthorItem(Item):\n",
    "    institution = Field()\n",
    "    email = Field()\n",
    "    avatar = Field()\n",
    "    vitae = Field()\n",
    "    fname = Field()\n",
    "    lname = Field()\n",
    "    address = Field()\n",
    "\n",
    "    title = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdate(dates):\n",
    "    d = {}\n",
    "    d['submission_date'] = None\n",
    "    d['revision_date'] = None\n",
    "    d['accepted_date'] = None\n",
    "    d['online_date'] = None\n",
    "    for date in dates:\n",
    "        if 'Received' in date:\n",
    "            d['submission_date'] = parse(date.split('Received ')[-1])\n",
    "        elif 'Revised' in date:\n",
    "            d['revision_date'] = parse(date.split('Revised ')[-1])\n",
    "        elif 'Accepted' in date:\n",
    "            d['accepted_date'] = parse(date.split('Accepted ')[-1])\n",
    "        elif 'Available online' in date:\n",
    "            d['online_date'] = parse(date.split('Available online ')[-1])\n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = dict(\n",
    "    title = './/h1[@class=\"chaptertitle\"]/text()',\n",
    "    abstract = './/div[@class=\"abstractSection abstractInFull\"]/p/text()',\n",
    "    submission_path = './/ul[@class=\"breadcrumbs\"]/li/a/text()',\n",
    "    revision_date = './/*[@class=\"publicationContentReceivedDate dates\"]/text()',\n",
    "    accepted_date = './/*[@class=\"publicationContentAcceptedDate dates\"]/text()',\n",
    "    online_date = './/*[@class=\"publicationContentEpubDate dates\"]/text()',\n",
    "    pages = './/div[@class=\"publicationContentPageRange\"]/text()'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ItemLoader(item = DocumentItem(), response = response)\n",
    "l.default_output_processor = TakeFirst()\n",
    "\n",
    "l.get_xpath(document['title'])[0].replace('\\n', '').strip()\n",
    "l.get_xpath(document['abstract'])[0]\n",
    "l.get_xpath(document['pages'])[0].split('\\n')[-2].strip().split(' - ')\n",
    "l.get_xpath(document['submission_path'])[-1].replace('\\n', '').strip()\n",
    "\n",
    "parse(l.get_xpath(document['online_date'])[0].replace('\\n', '').strip().split('Published Online: ')[-1])\n",
    "parse(l.get_xpath(document['accepted_date'])[0].replace('\\n', '').strip().split('Accepted: ')[-1])\n",
    "parse(l.get_xpath(document['revision_date'])[0].replace('\\n', '').strip().split('Received: ')[-1])\n",
    "\n",
    "pages = l.get_xpath(document['pages'])[0].split('\\n')[-2].strip().split(' - ')\n",
    "pages[0]\n",
    "pages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from scrapy.loader import ItemLoader\n",
    "from scrapy.loader.processors import Join, TakeFirst\n",
    "\n",
    "def load_document(response, document):\n",
    "    l = ItemLoader(item = DocumentItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    \n",
    "    l.add_value('title', l.get_xpath(document['title'])[0].replace('\\n', '').strip())\n",
    "    l.add_value('abstract', l.get_xpath(document['abstract'])[0])\n",
    "    l.add_value('submission_path', l.get_xpath(document['submission_path'])[-1].replace('\\n', '').strip())\n",
    "\n",
    "    # dates\n",
    "    try:\n",
    "        l.add_value('online_date', parse(l.get_xpath(document['online_date'])[0].replace('\\n', '').strip().split('Published Online: ')[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        l.add_value('accepted_date', parse(l.get_xpath(document['accepted_date'])[0].replace('\\n', '').strip().split('Accepted: ')[-1]))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        l.add_value('revision_date', parse(l.get_xpath(document['revision_date'])[0].replace('\\n', '').strip().split('Received: ')[-1]))\n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    # handle pages\n",
    "    try:\n",
    "        pages = l.get_xpath(document['pages'])[0].split('\\n')[-2].strip().split(' - ')\n",
    "        fp = int(pages[0])\n",
    "        lp = int(pages[-1])\n",
    "        l.add_value('fpage', fp)\n",
    "        l.add_value('lpage', lp)\n",
    "        l.add_value('pages', lp-fp+1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # mark it down, with source's publication_title\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_document(response, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = './/*[@class=\"abstractKeywords\"]//a/text()'\n",
    "\n",
    "response.xpath(keyword).extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = dict(\n",
    "    name = './/div[@class=\"contribDegrees\"]/a[@class=\"entryAuthor\"]/text()',\n",
    "    institution = './/*[@class=\"contribAff\"]/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = response.xpath(author['name']).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions = response.xpath(author['institution']).extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_author(response, author):\n",
    "    names = response.xpath(author['name']).extract()\n",
    "    institutions = response.xpath(author['institution']).extract()\n",
    "    for i in range(len(names)):\n",
    "        name = names[i].split()\n",
    "        fn = name[0]\n",
    "        ln = name[-1]\n",
    "        institution = institutions[i]\n",
    "        l = ItemLoader(item = AuthorItem(), response = response)\n",
    "        l.default_output_processor = TakeFirst()\n",
    "        l.add_value('fname', fn)\n",
    "        l.add_value('lname', ln)\n",
    "        l.add_value('institution', institution)\n",
    "        yield l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in load_author(response, author):\n",
    "    print(l.load_item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://pubsonline.informs.org/journal/mnsc', \n",
    "                 headers = headers)\n",
    "\n",
    "response = TextResponse(r.url, body = r.text, encoding = 'utf-8')\n",
    "\n",
    "# there is a response we need to handle\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = dict(\n",
    "    issn = './/div[@class=\"wrapped \"]/div/div[@class=\"pb-rich-text\"]/p/span/text()',\n",
    "    publication_title = './/ul[@class=\"breadcrumbs\"]/li/text()',\n",
    "    description = './/div[@class=\"pb-rich-text\"]/p',\n",
    "    coverimage = './/div[@class=\"pb-columns row-fluid \"]//div[@class=\"wrapped \"]/div/a/img/@src'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description\n",
    "description = unicodedata.normalize(\"NFKD\", cleanhtml(response.xpath(source['description']).extract()[1]))\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issn \n",
    "issn = [i for i in response.xpath(source['issn']).extract() if \"ISSN: \" in i][0].replace('ISSN: ', '')\n",
    "issn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publication_title = response.xpath(source['publication_title']).extract()[-1].replace('\\n', '').strip()\n",
    "publication_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informs_url = \"http://pubsonline.informs.org\"\n",
    "coverimage = informs_url + response.xpath(source['coverimage']).extract()[-1]\n",
    "coverimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scrapy.loader.processors import Join, TakeFirst, Join\n",
    "import unicodedata\n",
    "\n",
    "# this function is used to strip the html tags\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def load_source(response, source):\n",
    "    informs_url = \"http://pubsonline.informs.org\"\n",
    "    coverimage = informs_url + response.xpath(source['coverimage']).extract()[-1]\n",
    "    publication_title = response.xpath(source['publication_title']).extract()[-1].replace('\\n', '').strip()\n",
    "    issn = [i for i in response.xpath(source['issn']).extract() if \"ISSN: \" in i][0].replace('ISSN: ', '')\n",
    "    description = unicodedata.normalize(\"NFKD\", cleanhtml(response.xpath(source['description']).extract()[1]))\n",
    "    \n",
    "    l = ItemLoader(item = SourceItem(), response = response)\n",
    "    l.default_output_processor = TakeFirst()\n",
    "    l.add_value(\"issn\", issn)\n",
    "    l.add_value('publication_title', publication_title)\n",
    "    l.add_value('coverimage', coverimage)\n",
    "    l.add_value('description', description)\n",
    "    l.add_value('home_url', response.url)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = load_source(response, source)\n",
    "l.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
